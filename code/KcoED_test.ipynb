{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 진행상황\n",
    "\n",
    "- 중성색에 대한 알고리즘은 아직 구현하지 아니함.\n",
    "- KOTE 감정범주화에 대응하는 새로운 색상값을 라벨링하는 것으로 연구 방향을 정함.\n",
    "- 106개의 IRI 이미지 형용사는 아직 정리하지 아니함\n",
    "- Blossom 프롬프트 엔지니어링 해야함. inpu text에 대한 일관성 부족"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. color mapping\n",
    "#### context engineering 전, dummy를 가지고 색 선정 함수를 정의하고자함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import random\n",
    "import colorsys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- KOTE 44가지 감정의 긍/부정/중립 기준 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_emotions = ['감동/감탄', '고마움', '기대감', '기쁨', '뿌듯함',\n",
    "'신기함/관심', '아껴주는', '안심/신뢰', '존경', '즐거움/신남', '편안/쾌적', '행복', '환영/호의',\n",
    "'흐뭇함(귀여움/예쁨)',\n",
    "]\n",
    "negative_emotions = ['경악', '공포/무서움', '귀찮음', '당황/난처', '부끄러움', '부담/안_내킴', '불쌍함/연민', '불안/걱정', \n",
    "                     '불평/불만', '슬픔', '서러움', '안타까움/실망', '어이없음', '역겨움/징그러움', '의심/불신', '짜증', \n",
    "                     '재미없음', '절망', '죄책감', '증오/혐오', '지긋지긋', '패배/자기혐오', '한심함', '화남/분노', '힘듦/지침', '분노']\n",
    "neutral_emotions = ['깨달음', '놀람', '비장함', '우쭐댐/무시함']\n",
    "\n",
    "no_emotion = ['없음']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_emotions_contrast = {'불쌍함/연민': 0.44,\n",
    "'불안/걱정': 0.56,\n",
    "'서러움': 0.51,\n",
    "'슬픔': 0.7,\n",
    "'아껴주는' : 0.47,\n",
    "'안타까움/실망': 0.31,\n",
    "'없음': 0.8,\n",
    "'기쁨':0.6\n",
    "}\n",
    "\n",
    "result_emotions_similar = {'불쌍함/연민': 0.44,\n",
    "'불안/걱정': 0.56,\n",
    "'서러움': 0.51,\n",
    "'슬픔': 0.7,\n",
    "'아껴주는' : 0.47,\n",
    "'안타까움/실망': 0.31,\n",
    "'없음': 0.8,\n",
    "'죄책감':0.6\n",
    "}\n",
    "\n",
    "result_emotions_similaremotion_contrastcolor = {'불쌍함/연민': 0.44,\n",
    "'불안/걱정': 0.56,\n",
    "'서러움': 0.51,\n",
    "'슬픔': 0.7,\n",
    "'아껴주는' : 0.47,\n",
    "'안타까움/실망': 0.31,\n",
    "'없음': 0.8,\n",
    "'분노':0.6\n",
    "}\n",
    "\n",
    "result_IRI = '사랑스러운'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KCoED 데이터셋 & IRI 데이터셋 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV 파일 경로\n",
    "file_path = \"../data/korean emtion-color dataset 2.0 - SUM.csv\"\n",
    "# 파일 읽기 (처음엔 컬럼명을 무시하고 불러오기)\n",
    "df_raw = pd.read_csv(file_path, header=None)\n",
    "\n",
    "# 첫 번째 행을 컬럼명으로 지정\n",
    "df_raw.columns = df_raw.iloc[1]\n",
    "df = df_raw[2:]  # 첫 행은 이제 컬럼으로 사용했으니 제외\n",
    "\n",
    "# CSV 파일 경로\n",
    "file_path_IRI = \"../data/임이로_학위논문_korean emtion-color dataset 2.0 - IRI-형용사스케일.csv\"\n",
    "# 파일 읽기 (처음엔 컬럼명을 무시하고 불러오기)\n",
    "df_raw_IRI = pd.read_csv(file_path_IRI, header=None)\n",
    "\n",
    "# 첫 번째 행을 컬럼명으로 지정\n",
    "df_raw_IRI.columns = df_raw_IRI.iloc[1]\n",
    "df_IRI = df_raw_IRI[2:]  # 첫 행은 이제 컬럼으로 사용했으니 제외\n",
    "\n",
    "# CSV 파일 경로\n",
    "file_path_IRI_colors = \"../data/임이로_학위논문_korean emtion-color dataset 2.0 - IRI-배색스케일.csv\"\n",
    "# 파일 읽기 (처음엔 컬럼명을 무시하고 불러오기)\n",
    "df_raw_IRI_colors = pd.read_csv(file_path_IRI_colors, header=None)\n",
    "\n",
    "# 첫 번째 행을 컬럼명으로 지정\n",
    "df_raw_IRI_colors.columns = df_raw_IRI_colors.iloc[1]\n",
    "df_IRI_colors = df_raw_IRI_colors[2:]  # 첫 행은 이제 컬럼으로 사용했으니 제외\n",
    "\n",
    "# ----------------------------------------\n",
    "# 색상약호가 'White' 또는 'Black'인 행 제거\n",
    "# ----------------------------------------\n",
    "\n",
    "# '색상약호' 컬럼에 NaN이 있을 수 있으므로 fillna 처리\n",
    "df_IRI_colors = df_IRI_colors[~df_IRI_colors['색상약호'].fillna('').isin(['White', 'Black'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3897/2318276721.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: list(g['형용사']))\n"
     ]
    }
   ],
   "source": [
    "# 필요한 열만 추출 ('형용사군', '형용사id', '형용사')\n",
    "adjective_df = df_IRI[['형용사군', '형용사id', '형용사']].dropna()\n",
    "\n",
    "# 형용사id를 숫자로 정리 (필요한 경우)\n",
    "adjective_df['형용사id'] = adjective_df['형용사id'].astype(int)\n",
    "\n",
    "# 형용사군별로 그룹화하여 리스트로 변환\n",
    "grouped_adjectives = (\n",
    "    adjective_df.groupby('형용사군')\n",
    "    .apply(lambda g: list(g['형용사']))\n",
    "    .to_dict()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 주감정/보조감정에 대해 KCoED데이터셋에서 주색, 보조색을 선정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 감정 출력 값 전처리 : 없음을 제거, 감정 범주화에 해당하지 않는 감정 제거 ex) 한심함\n",
    "def filter_emotions(result_emotions, result_IRI):\n",
    "    # Filter out the '없음' key\n",
    "    filtered_emotions = {key: value for key, value in result_emotions.items() if key != '없음'}\n",
    "    \n",
    "    # Sort emotions by their values in descending order\n",
    "    sorted_emotions = sorted(filtered_emotions.items(), key=lambda item: item[1], reverse=True)\n",
    "    \n",
    "    # Extract the keys of the top two emotions\n",
    "    top_two_emotions = [sorted_emotions[0][0], sorted_emotions[1][0], result_IRI] if len(sorted_emotions) >= 2 else []\n",
    "    \n",
    "    return top_two_emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['슬픔', '기쁨', '사랑스러운'] ['슬픔', '죄책감', '사랑스러운'] ['슬픔', '분노', '사랑스러운']\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "## 주-보조 감정은 감정 분류 모델(KPoEM)에서, 형용사는 LLM모델이 반환.\n",
    "top_emotions_contrast = filter_emotions(result_emotions_contrast, result_IRI)\n",
    "top_emotions_similar = filter_emotions(result_emotions_similar, result_IRI)\n",
    "top_emotions_similaremotion_contrastcolor = filter_emotions(result_emotions_similaremotion_contrastcolor, result_IRI)\n",
    "print(top_emotions_contrast, top_emotions_similar, top_emotions_similaremotion_contrastcolor)  # Output: ['슬픔', '기쁨']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to categorize the emotion\n",
    "def categorize_emotion(primary_emotion, secondary_emotion):\n",
    "    # 감정 사전 정의 (예시, 필요에 따라 확장 가능)\n",
    "\n",
    "    # 내부 분류 함수\n",
    "    def classify(emotion):\n",
    "        if emotion in positive_emotions:\n",
    "            return \"positive\"\n",
    "        elif emotion in negative_emotions:\n",
    "            return \"negative\"\n",
    "        elif emotion in neutral_emotions:\n",
    "            return \"neutral\" # 아직 처리 안함.\n",
    "        else:\n",
    "            return \"no_emotion\" #아직 처리 안함.\n",
    "    \n",
    "    def handle_neutral_case(primary_emotion, secondary_emotion):\n",
    "        # 감정이 'neutral'인 경우는 별도의 로직으로 처리\n",
    "        if primary_emotion == \"neutral\" or secondary_emotion == \"neutral\":\n",
    "            return \"배색3\"\n",
    "\n",
    "    # 각각 분류\n",
    "    primary_category = classify(primary_emotion)\n",
    "    secondary_category = classify(secondary_emotion)\n",
    "    \n",
    "    # 'neutral'이 포함된 경우는 별도 함수로 처리\n",
    "    if \"neutral\" in [primary_category, secondary_category] :\n",
    "        print(f\"Primary: {primary_category}, Secondary: {secondary_category}\")\n",
    "        return handle_neutral_case(primary_category, secondary_category)\n",
    "    \n",
    "    # 조건 비교 후 mode 반환\n",
    "    if primary_category == secondary_category in [\"positive\", \"negative\"]:\n",
    "        mode1 = \"유사색상\"\n",
    "        print(f\"Primary: {primary_category}, Secondary: {secondary_category}, Mode : {mode1}\")\n",
    "        return \"유사색상\"\n",
    "    else:\n",
    "        mode1 = \"유사색상\"\n",
    "        print(f\"Primary: {primary_category}, Secondary: {secondary_category}, Mode : {mode1}\")\n",
    "        return \"대비색상\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_color(emotion, df):\n",
    "    filtered = df[(df['감정 어휘'] == emotion) & (df['평가 순위'] == '1')]\n",
    "    filtered_df_array = filtered.to_dict(orient='records')\n",
    "\n",
    "    # (예외처리) 무작위로 하나 선택\n",
    "    if filtered_df_array:\n",
    "        return random.choice(filtered_df_array)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_color_info(top_ranked_value):\n",
    "    munsell_colors = ['R', 'YR', 'Y', 'GY', 'G', 'BG', 'B', 'PB', 'P', 'RP']\n",
    "    total = len(munsell_colors)\n",
    "\n",
    "    if top_ranked_value['색상약호'] not in munsell_colors:\n",
    "        return \"unknown_color\"\n",
    "    # 색상약호의 인덱스 찾기\n",
    "    idx = munsell_colors.index(top_ranked_value['색상약호'])\n",
    "\n",
    "    # 유사색상군\n",
    "    similar_1_idx = [(idx - 1) % total, (idx + 1) % total]\n",
    "    similar_2_idx = [(idx - 2) % total, (idx + 2) % total]\n",
    "\n",
    "    # 대비색상군\n",
    "    contrast_2_idx = [(idx - 3) % total, (idx + 3) % total]\n",
    "    contrast_1_idx = [(idx - 4) % total, (idx + 4) % total]\n",
    "    contrast_idx = [(idx - 5) % total]  # 보색 (정반대 5칸 거리)\n",
    "\n",
    "    # 변환\n",
    "    similar_colors_1 = [munsell_colors[i] for i in similar_1_idx]\n",
    "    similar_colors_2 = [munsell_colors[i] for i in similar_2_idx]\n",
    "    contrast_colors_2 = [munsell_colors[i] for i in contrast_2_idx]\n",
    "    contrast_colors_1 = [munsell_colors[i] for i in contrast_1_idx]\n",
    "    contrast_color = [munsell_colors[i] for i in contrast_idx]\n",
    "\n",
    "    return {\n",
    "        \"center_color\": top_ranked_value,\n",
    "        \"similar_colors_1\": similar_colors_1,\n",
    "        \"similar_colors_2\": similar_colors_2,\n",
    "        \"contrast_colors_1\": contrast_colors_1,\n",
    "        \"contrast_colors_2\": contrast_colors_2,\n",
    "        \"contrast_color\": contrast_color  # 보색\n",
    "    }\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar_mapping(primary_color_info, secondary_emotion, df):\n",
    "    # secondary_ranked_value = df[df['감정 어휘'] == secondary_emotion].sort_values(by='평가 순위').iloc[0].to_dict()\n",
    "    secondary_ranked_df = df[df['감정 어휘'] == secondary_emotion]\n",
    "    \n",
    "    similar_matches_1 = []\n",
    "    similar_matches_2 = []\n",
    "    similar_match_exact = []\n",
    "\n",
    "            \n",
    "    # similar_colors_1 체크\n",
    "    for similar_color in primary_color_info['similar_colors_1']:\n",
    "        matched = secondary_ranked_df[secondary_ranked_df['색상약호'] == similar_color]\n",
    "        if not matched.empty:\n",
    "            similar_matches_1.append(matched)\n",
    "\n",
    "    # similar_colors_2 체크\n",
    "    for similar_color in primary_color_info['similar_colors_2']:\n",
    "        matched = secondary_ranked_df[secondary_ranked_df['색상약호'] == similar_color]\n",
    "        if not matched.empty:\n",
    "            similar_matches_2.append(matched)\n",
    "            \n",
    "    # same_color (동일색) 체크\n",
    "    same_color = primary_color_info['center_color']  # 단일 값 리스트 #주색은 미리 딕셔너리로 보관됨. 그대로 가져옴.\n",
    "    matched_exact = secondary_ranked_df[secondary_ranked_df['색상약호'] == same_color]\n",
    "    if not matched_exact.empty:\n",
    "        similar_match_exact.append(matched_exact)\n",
    "    \n",
    "    # 유사색상군1과 유사색상군2 중 하나라도 일치하는 경우.\n",
    "    if similar_matches_1 or similar_matches_2: \n",
    "        if similar_matches_1: # 대비색상군1을 우선 선정.\n",
    "            similar_matches_1_df = pd.concat(similar_matches_1).to_dict(orient='records')\n",
    "            # print(similar_matches_1_df)\n",
    "            return {\"primary_color\" : primary_color_info['center_color'],\n",
    "                    \"secondary_color\" : min(similar_matches_1_df, key=lambda x: x['평가 순위'])}\n",
    "        else: #대비색상군1이 없고, 대비색상군2가 있는 경우\n",
    "            similar_matches_2_df = pd.concat(similar_matches_2).to_dict(orient='records')\n",
    "            # print(similar_matches_2_df)\n",
    "            return {\"primary_color\" : primary_color_info['center_color'],\n",
    "                    \"secondary_color\" :min(similar_matches_2_df, key=lambda x: x['평가 순위'])}\n",
    "            \n",
    "    # 동일색이 있는경우\n",
    "    if similar_match_exact:\n",
    "        similar_dicts = similar_match_exact[0].to_dict(orient='records')\n",
    "        return {\"primary_color\" : primary_color_info['center_color'],\n",
    "                \"secondary_color\" : min(similar_dicts, key=lambda x: x['평가 순위'])} # '평가 순위'가 가장 높은 값 (숫자가 클수록 높은 순위라면 ↓)\n",
    "        \n",
    "    # 유사색상이 없는 경우\n",
    "    pure_secondary_colors = get_color(secondary_emotion, df)\n",
    "    return {\"primary_color\" : primary_color_info['center_color'], \"secondary_color\" :pure_secondary_colors}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrast_mapping(primary_color_info, secondary_emotion, df):\n",
    "    secondary_ranked_df = df[df['감정 어휘'] == secondary_emotion]\n",
    "        \n",
    "    contrast_matches_1 = []\n",
    "    contrast_matches_2 = []\n",
    "    contrast_match_exact = []\n",
    "\n",
    "            \n",
    "    # contrast_colors_1 체크\n",
    "    for contrast_color in primary_color_info['contrast_colors_1']:\n",
    "        matched = secondary_ranked_df[secondary_ranked_df['색상약호'] == contrast_color]\n",
    "        if not matched.empty:\n",
    "            contrast_matches_1.append(matched)\n",
    "\n",
    "    # contrast_colors_2 체크\n",
    "    for contrast_color in primary_color_info['contrast_colors_2']:\n",
    "        matched = secondary_ranked_df[secondary_ranked_df['색상약호'] == contrast_color]\n",
    "        if not matched.empty:\n",
    "            contrast_matches_2.append(matched)\n",
    "            \n",
    "    # contrast_color (보색) 체크\n",
    "    contrast_color = primary_color_info['contrast_color'][0]  # 단일 값 리스트\n",
    "    matched_exact = secondary_ranked_df[secondary_ranked_df['색상약호'] == contrast_color]\n",
    "    if not matched_exact.empty:\n",
    "        contrast_match_exact.append(matched_exact)\n",
    "    \n",
    "    #보색이 정확히 일치하는 경우\n",
    "    if contrast_match_exact:\n",
    "        contrast_dicts = contrast_match_exact[0].to_dict(orient='records')\n",
    "        return {\"primary_color\" : primary_color_info['center_color'],\n",
    "                \"secondary_color\" : min(contrast_dicts, key=lambda x: x['평가 순위'])} # '평가 순위'가 가장 높은 값 (숫자가 클수록 높은 순위라면 ↓)\n",
    "    \n",
    "    # 대비색상군1과 대비색상군2 중 하나라도 일치하는 경우.\n",
    "    if contrast_matches_1 or contrast_matches_2: \n",
    "        if contrast_matches_1: # 대비색상군1을 우선 선정.\n",
    "            contrast_matches_1_df = pd.concat(contrast_matches_1).to_dict(orient='records')\n",
    "            # print(contrast_matches_1_df)\n",
    "            return {\"primary_color\" : primary_color_info['center_color'],\n",
    "                    \"secondary_color\" : min(contrast_matches_1_df, key=lambda x: x['평가 순위'])}\n",
    "        else: #대비색상군1이 없고, 대비색상군2가 있는 경우\n",
    "            contrast_matches_2_df = pd.concat(contrast_matches_2).to_dict(orient='records')\n",
    "            # print(contrast_matches_2_df)\n",
    "            return {\"primary_color\" : primary_color_info['center_color'],\n",
    "                    \"secondary_color\" :min(contrast_matches_2_df, key=lambda x: x['평가 순위'])}\n",
    "    # 대비색상이 없는 경우\n",
    "    pure_secondary_colors = get_color(secondary_emotion, df)\n",
    "    return {\"primary_color\" : primary_color_info['center_color'], \"secondary_color\" :pure_secondary_colors}  # '평가 순위'가 가장 높은 값 (숫자가 클수록 높은 순위라면 ↓)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sencondary_color(primary_color_info, secondary_emotion, mode_colors, df):\n",
    "    # 유사색상 모드일 때\n",
    "    if mode_colors == \"유사색상\":\n",
    "        return similar_mapping(primary_color_info, secondary_emotion, df)\n",
    "    # 대비색상 모드일 때\n",
    "    elif mode_colors == \"대비색상\": \n",
    "        return contrast_mapping(primary_color_info, secondary_emotion, df)\n",
    "        \n",
    "    # 예외처리 : 대비색상, 대비색상군1, 대비색상군2가 전부 없는 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def define_colors(primary_emotion, secondary_emotion, df):\n",
    "#     mode_colors = categorize_emotion(primary_emotion, secondary_emotion)\n",
    "#     # top_ranked_value = df[df['감정 어휘'] == primary_emotion].sort_values(by='평가 순위').iloc[0].to_dict()\n",
    "#     # top_ranked_value = df[df['감정 어휘'] == primary_emotion].sort_values(by='평가 순위')\n",
    "#     # filtered = df[(df['감정 어휘'] == primary_emotion) & (df['평가 순위'] == 1)]\n",
    "    \n",
    "#     primary_color_data = get_color(primary_emotion, df) # 무작위로 하나 선택\n",
    "#     primary_color_info_test= get_primary_color(primary_color_data)\n",
    "#     return get_sencondary_color(primary_color_info_test, secondary_emotion, mode_colors, df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colors_similar = define_colors(top_emotions_similar[0],top_emotions_similar[1], df) # 주감정과 보조감정, KCoED 데이터셋\n",
    "# colors_similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colors_similar_contrast = define_colors(top_emotions_similaremotion_contrastcolor[0],top_emotions_similaremotion_contrastcolor[1], df) # 주감정과 보조감정, KCoED 데이터셋\n",
    "# colors_similar_contrast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 주색/보조색을 가지고 컬러 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb_to_hsb(rgb_dict):\n",
    "    # 문자열 RGB 값을 정수로 변환\n",
    "    r = int(rgb_dict['R']) / 255.0\n",
    "    g = int(rgb_dict['G']) / 255.0\n",
    "    b = int(rgb_dict['B']) / 255.0\n",
    "\n",
    "    # RGB → HSB (HSV)\n",
    "    h, s, v = colorsys.rgb_to_hsv(r, g, b)\n",
    "\n",
    "    # HSB 값 반환 (0~360, 0~100, 0~100)\n",
    "    return {\n",
    "        'H': round(h * 360), #반올림\n",
    "        'S': round(s * 100),\n",
    "        'B': round(v * 100),\n",
    "        'color info' : rgb_dict\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_primary_color1 = rgb_to_hsb(colors_similar['primary_color'])\n",
    "# processed_secondary_color1 = rgb_to_hsb(colors_similar['secondary_color'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_primary_color2 = rgb_to_hsb(colors_contrast['primary_color'])\n",
    "# processed_secondary_color2 = rgb_to_hsb(colors_contrast['secondary_color'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_primary_color3 = rgb_to_hsb(colors_similar_contrast['primary_color'])\n",
    "# processed_secondary_color3 = rgb_to_hsb(colors_similar_contrast['secondary_color'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 형용사 정리 (버그수정)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 추출한 감정형용사의 형용사군 검색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_adjective_group(adjective, grouped_dict):\n",
    "    \"\"\"\n",
    "    주어진 형용사가 속한 형용사군(key)을 반환.\n",
    "    \"\"\"\n",
    "    for group, adjectives in grouped_dict.items():\n",
    "        if adjective in adjectives:\n",
    "            return group\n",
    "    return None  # 못 찾았을 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "형용사 '사랑스러운'는 '귀여운' 형용사군에 속합니다.\n"
     ]
    }
   ],
   "source": [
    "group_name = find_adjective_group(result_IRI, grouped_adjectives)\n",
    "\n",
    "if group_name:\n",
    "    print(f\"형용사 '{result_IRI}'는 '{group_name}' 형용사군에 속합니다.\")\n",
    "else:\n",
    "    print(f\"형용사 '{result_IRI}'는 어떤 형용사군에도 속하지 않습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "선택된 배색그룹id: 1\n"
     ]
    }
   ],
   "source": [
    "IRI_colors_df = df_IRI_colors[df_IRI_colors['형용사군'] == group_name]\n",
    "# '배색그룹id' 컬럼에서 고유값만 추출\n",
    "palette_ids = IRI_colors_df['배색그룹id'].dropna().unique()\n",
    "\n",
    "# 랜덤 선택\n",
    "if len(palette_ids) > 0:\n",
    "    chosen_palette_id = random.choice(palette_ids)\n",
    "    print(f\"선택된 배색그룹id: {chosen_palette_id}\")\n",
    "else:\n",
    "    chosen_palette_id = None\n",
    "    print(\"배색그룹id가 존재하지 않습니다.\")\n",
    "    \n",
    "# chosen_palette_id에 해당하는 행만 추출\n",
    "palette_df = IRI_colors_df[IRI_colors_df['배색그룹id'] == chosen_palette_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_color_pair(palette_df):\n",
    "    # 사용할 쌍 정의 (우선순위 순서)\n",
    "    priority_pairs = [(1, 2), (2, 3), (2, 1), (3, 2)]\n",
    "    fallback_pairs = [(1, 3), (3, 1)]\n",
    "\n",
    "    # 현재 데이터프레임에 존재하는 단일색상 id 추출\n",
    "    available_ids = set(palette_df['단일색상 id'].dropna().astype(int).unique())\n",
    "\n",
    "    # 가능한 쌍 필터링\n",
    "    valid_priority_pairs = [pair for pair in priority_pairs if pair[0] in available_ids and pair[1] in available_ids]\n",
    "    valid_fallback_pairs = [pair for pair in fallback_pairs if pair[0] in available_ids and pair[1] in available_ids]\n",
    "\n",
    "    # 랜덤 선택\n",
    "    if valid_priority_pairs:\n",
    "        return random.choice(valid_priority_pairs)\n",
    "    elif valid_fallback_pairs:\n",
    "        return random.choice(valid_fallback_pairs)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_palette_dicts(palette_df):\n",
    "    \"\"\"\n",
    "    palette_result 튜플 (id1, id2)에 대해 해당하는 단일색상id 행을\n",
    "    각각 dictionary로 반환\n",
    "    \"\"\"\n",
    "    palette_result = choose_color_pair(palette_df)\n",
    "    id1, id2 = palette_result\n",
    "\n",
    "    # '단일색상 id' 컬럼이 float일 가능성 있으므로 int로 캐스팅\n",
    "    df_cast = palette_df.copy()\n",
    "    df_cast['단일색상 id'] = df_cast['단일색상 id'].astype(int)\n",
    "\n",
    "    # 각 색상 id에 대응하는 행 추출 후 dict 변환\n",
    "    primary_row = df_cast[df_cast['단일색상 id'] == id1].iloc[0].to_dict()\n",
    "    secondary_row = df_cast[df_cast['단일색상 id'] == id2].iloc[0].to_dict()\n",
    "\n",
    "    return {'primary_palette' : primary_row, 'secondary_palette' : secondary_row}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette_result = get_palette_dicts(palette_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_saturation_brightness(processed_primary_color, processed_secondary_color, palette_result):\n",
    "    \"\"\"\n",
    "    processed_primary_color와 processed_secondary_color의 S, B 값을\n",
    "    palette_result의 S, B 값으로 업데이트합니다.\n",
    "    \"\"\"\n",
    "    processed_primary_color['S'] = palette_result['primary_palette']['Saturation']\n",
    "    processed_primary_color['B'] = palette_result['primary_palette']['Brightness']\n",
    "    processed_secondary_color['S'] = palette_result['secondary_palette']['Saturation']\n",
    "    processed_secondary_color['B'] = palette_result['secondary_palette']['Brightness']\n",
    "\n",
    "    return {'updated_primary_color': processed_primary_color,\n",
    "            'updated_secondary_color': processed_secondary_color\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updated_colors1 = update_saturation_brightness(processed_primary_color1, processed_secondary_color1, palette_result)\n",
    "# updated_colors2 = update_saturation_brightness(processed_primary_color2, processed_secondary_color2, palette_result)\n",
    "# updated_colors3 = update_saturation_brightness(processed_primary_color3, processed_secondary_color3, palette_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 감정 어휘 정리 (진행예정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_emotion_text(emotion_name):\n",
    "    if emotion_name == '기대감' :\n",
    "        return '기대하다'\n",
    "    else : \n",
    "        return emotion_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def define_colors(result_emotions, result_IRI, df, IRI_colors_df):\n",
    "    top_two_emotions = filter_emotions(result_emotions, result_IRI)\n",
    "    mode1 = categorize_emotion(top_two_emotions[0],top_two_emotions[1])\n",
    "    filtered_top_two_emotion = filter_emotion_text(top_two_emotions[0]) # 임시적용\n",
    "    primary_color = get_color(filtered_top_two_emotion, df) # 무작위로 하나 선택\n",
    "    primary_color_info_data = get_color_info(primary_color)\n",
    "    \n",
    "    selected_colors = get_sencondary_color(primary_color_info_data, top_two_emotions[1], mode1, df)\n",
    "    processed_primary_color = rgb_to_hsb(selected_colors['primary_color'])\n",
    "    processed_secondary_color = rgb_to_hsb(selected_colors['secondary_color'])\n",
    "    \n",
    "    group_name = find_adjective_group(result_IRI, grouped_adjectives)\n",
    "    if group_name:\n",
    "        print(f\"형용사 '{result_IRI}'는 '{group_name}' 형용사군에 속합니다.\")\n",
    "    else:\n",
    "        print(f\"형용사 '{result_IRI}'는 어떤 형용사군에도 속하지 않습니다.\")\n",
    "        \n",
    "    IRI_colors_df = df_IRI_colors[df_IRI_colors['형용사군'] == group_name]\n",
    "    # '배색그룹id' 컬럼에서 고유값만 추출\n",
    "    palette_ids = IRI_colors_df['배색그룹id'].dropna().unique()\n",
    "\n",
    "    # 랜덤 선택\n",
    "    if len(palette_ids) > 0:\n",
    "        chosen_palette_id = random.choice(palette_ids)\n",
    "        print(f\"선택된 배색그룹id: {chosen_palette_id}\")\n",
    "    else:\n",
    "        chosen_palette_id = None\n",
    "        print(\"배색그룹id가 존재하지 않습니다.\")\n",
    "    \n",
    "    # chosen_palette_id에 해당하는 행만 추출\n",
    "    palette_df = IRI_colors_df[IRI_colors_df['배색그룹id'] == chosen_palette_id]    \n",
    "    palette_result = get_palette_dicts(palette_df)\n",
    "    updated_colors = update_saturation_brightness(processed_primary_color, processed_secondary_color, palette_result)\n",
    "    updated_colors['primary_emotion'] = top_two_emotions[0]\n",
    "    updated_colors['secondary_emotion'] = top_two_emotions[1]\n",
    "    return updated_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primary: negative, Secondary: positive, Mode : 유사색상\n",
      "형용사 '사랑스러운'는 '귀여운' 형용사군에 속합니다.\n",
      "선택된 배색그룹id: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'updated_primary_color': {'H': 214,\n",
       "  'S': '58',\n",
       "  'B': '96',\n",
       "  'color info': {'감정id': '24',\n",
       "   '감정 어휘': '슬픔',\n",
       "   '평가 순위': '1',\n",
       "   'R': '76',\n",
       "   'G': '87',\n",
       "   'B': '101',\n",
       "   '샘플': nan,\n",
       "   '출처/참고문헌': '김애경,and 오윤경. \"긍정 및 부정 정서어휘에 대한 색 반응 연구.\" 조형미디어학 19.1 (2016): 59-66.',\n",
       "   '출처/참고문헌2': 'https://gist.github.com/hidex7777/5268753',\n",
       "   '색상': '남색',\n",
       "   '색상약호': 'PB',\n",
       "   '색상 온도': 'Cold',\n",
       "   nan: nan}},\n",
       " 'updated_secondary_color': {'H': 51,\n",
       "  'S': '49',\n",
       "  'B': '98',\n",
       "  'color info': {'감정id': '17',\n",
       "   '감정 어휘': '기쁨',\n",
       "   '평가 순위': '1',\n",
       "   'R': '238',\n",
       "   'G': '201',\n",
       "   'B': '0',\n",
       "   '샘플': nan,\n",
       "   '출처/참고문헌': '김애경,and 오윤경. \"긍정 및 부정 정서어휘에 대한 색 반응 연구.\" 조형미디어학 19.1 (2016): 59-66.',\n",
       "   '출처/참고문헌2': 'https://gist.github.com/hidex7777/5268700',\n",
       "   '색상': '노랑',\n",
       "   '색상약호': 'Y',\n",
       "   '색상 온도': 'Warm',\n",
       "   nan: nan}},\n",
       " 'primary_emotion': '슬픔',\n",
       " 'secondary_emotion': '기쁨'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colors_contrast = define_colors(result_emotions_contrast, result_IRI, df, IRI_colors_df) # 주감정과 보조감정, KCoED 데이터셋\n",
    "colors_contrast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hsb_to_rgb_tuple(hsb):\n",
    "    h = int(hsb['H']) / 360\n",
    "    s = int(hsb['S']) / 100\n",
    "    v = int(hsb['B']) / 100\n",
    "    r, g, b = colorsys.hsv_to_rgb(h, s, v)\n",
    "    return (r, g, b)  # matplotlib uses 0-1 float RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotion_to_colors(top_emotions, color_results):\n",
    "    plt.rcParams['font.family'] = 'NanumGothic'\n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "    # RGB 변환\n",
    "    rgb1 = hsb_to_rgb_tuple(color_results['updated_primary_color'])\n",
    "    rgb2 = hsb_to_rgb_tuple(color_results['updated_secondary_color'])\n",
    "\n",
    "    # Figure + GridSpec (7:3 비율)\n",
    "    fig = plt.figure(figsize=(7, 2))\n",
    "    gs = gridspec.GridSpec(1, 2, width_ratios=[7, 3])\n",
    "\n",
    "    # Primary Color 영역 (7)\n",
    "    ax1 = fig.add_subplot(gs[0])\n",
    "    ax1.imshow([[rgb1]], aspect='auto')\n",
    "    ax1.set_title(\"Primary Color\", fontsize=10)\n",
    "    ax1.axis('off')\n",
    "\n",
    "    # Secondary Color 영역 (3)\n",
    "    ax2 = fig.add_subplot(gs[1])\n",
    "    ax2.imshow([[rgb2]], aspect='auto')\n",
    "    ax2.set_title(\"Secondary Color\", fontsize=10)\n",
    "    ax2.axis('off')\n",
    "\n",
    "    # 전체 제목\n",
    "    # plt.suptitle(f\"{top_emotions[2]}, {top_emotions[0]}과(와) {top_emotions[1]}\", fontsize=14)\n",
    "    # 그래프 아래 중앙에 제목 넣기\n",
    "    fig.text(\n",
    "        0.5, -0.05,  # x, y (0~1 기준, 아래로 -0.05)\n",
    "        f\"{top_emotions[2]}, {top_emotions[0]}과(와) {top_emotions[1]}\",\n",
    "        ha='center', va='top',\n",
    "        fontsize=14\n",
    ")\n",
    "\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    plt.show()\n",
    "    \n",
    "    print(top_emotions, color_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x200 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['슬픔', '기쁨', '사랑스러운'] {'updated_primary_color': {'H': 214, 'S': '58', 'B': '96', 'color info': {'감정id': '24', '감정 어휘': '슬픔', '평가 순위': '1', 'R': '76', 'G': '87', 'B': '101', '샘플': nan, '출처/참고문헌': '김애경,and 오윤경. \"긍정 및 부정 정서어휘에 대한 색 반응 연구.\" 조형미디어학 19.1 (2016): 59-66.', '출처/참고문헌2': 'https://gist.github.com/hidex7777/5268753', '색상': '남색', '색상약호': 'PB', '색상 온도': 'Cold', nan: nan}}, 'updated_secondary_color': {'H': 51, 'S': '49', 'B': '98', 'color info': {'감정id': '17', '감정 어휘': '기쁨', '평가 순위': '1', 'R': '238', 'G': '201', 'B': '0', '샘플': nan, '출처/참고문헌': '김애경,and 오윤경. \"긍정 및 부정 정서어휘에 대한 색 반응 연구.\" 조형미디어학 19.1 (2016): 59-66.', '출처/참고문헌2': 'https://gist.github.com/hidex7777/5268700', '색상': '노랑', '색상약호': 'Y', '색상 온도': 'Warm', nan: nan}}, 'primary_emotion': '슬픔', 'secondary_emotion': '기쁨'}\n"
     ]
    }
   ],
   "source": [
    "# emotion_to_colors(top_emotions_similar, updated_colors1)\n",
    "# emotion_to_colors(top_emotions_contrast, updated_colors2)\n",
    "# emotion_to_colors(top_emotions_similaremotion_contrastcolor, updated_colors3)\n",
    "emotion_to_colors(top_emotions_contrast, colors_contrast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KPoEM 감정분류 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/bitsandbytes-0.45.4.dev0-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_utilities-0.12.0.dev0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/looseversion-1.3.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/nvfuser-0.2.23a0+6627725-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.0.dev0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: langchain in /home/work/.local/lib/python3.12/site-packages (0.3.27)\n",
      "Requirement already satisfied: langchain-community in /home/work/.local/lib/python3.12/site-packages (0.3.27)\n",
      "Requirement already satisfied: langchain-core in /home/work/.local/lib/python3.12/site-packages (0.3.74)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /home/work/.local/lib/python3.12/site-packages (from langchain) (0.3.9)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in /home/work/.local/lib/python3.12/site-packages (from langchain) (0.4.15)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.10.5)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.4.36)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.11.10)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /home/work/.local/lib/python3.12/site-packages (from langchain-community) (9.1.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /home/work/.local/lib/python3.12/site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /home/work/.local/lib/python3.12/site-packages (from langchain-community) (2.10.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /home/work/.local/lib/python3.12/site-packages (from langchain-community) (0.4.1)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (1.26.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/work/.local/lib/python3.12/site-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (4.12.2)\n",
      "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (23.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/work/.local/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/work/.local/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core) (2.3)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /home/work/.local/lib/python3.12/site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /home/work/.local/lib/python3.12/site-packages (from langsmith>=0.1.17->langchain) (0.24.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /home/work/.local/lib/python3.12/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/work/.local/lib/python3.12/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2024.12.14)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (3.7.1)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (0.4.3)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.2.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/bitsandbytes-0.45.4.dev0-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_utilities-0.12.0.dev0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/looseversion-1.3.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/nvfuser-0.2.23a0+6627725-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.0.dev0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: faiss-cpu in /home/work/.local/lib/python3.12/site-packages (1.12.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (23.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain langchain-community langchain-core\n",
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "import torch.nn as nn\n",
    "from transformers import ElectraModel, AutoTokenizer, AutoModel\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# 토크나이저 로드\n",
    "###########################\n",
    "MODEL_NAME = \"beomi/KcELECTRA-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1. 공통 전처리 함수 및 라벨 정의\n",
    "LABELS = ['불평/불만', '환영/호의', '감동/감탄', '지긋지긋', '고마움', '슬픔', '화남/분노', '존경', '기대감', '우쭐댐/무시함', '안타까움/실망', '비장함', '의심/불신', '뿌듯함', '편안/쾌적', '신기함/관심', '아껴주는', '부끄러움', '공포/무서움', '절망', '한심함', '역겨움/징그러움', '짜증', '어이없음', '없음', '패배/자기혐오', '귀찮음', '힘듦/지침', '즐거움/신남', '깨달음', '죄책감', '증오/혐오', '흐뭇함(귀여움/예쁨)', '당황/난처', '경악', '부담/안_내킴', '서러움', '재미없음', '불쌍함/연민', '놀람', '행복', '불안/걱정', '기쁨', '안심/신뢰']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.4. Pytorch Lightning 모델(BaseTagger) 정의\n",
    "class BaseTagger(pl.LightningModule):\n",
    "    def __init__(self, model_name=MODEL_NAME, lr=2e-5, weight_decay=0.01,\n",
    "                 n_training_steps=None, n_warmup_steps=None, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.electra = AutoModel.from_pretrained(model_name)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=self.hparams.dropout_rate),\n",
    "            nn.Linear(self.electra.config.hidden_size, len(LABELS))\n",
    "        )\n",
    "        self.criterion = nn.BCELoss()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        output = self.electra(input_ids, attention_mask=attention_mask)\n",
    "        logits = self.classifier(output.last_hidden_state[:, 0, :])\n",
    "        probs = torch.sigmoid(logits)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss = self.criterion(probs, labels)\n",
    "            return loss, probs\n",
    "        return None, probs\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, _ = self(**batch)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, _ = self(**batch)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.parameters(),\n",
    "            lr=self.hparams.lr,\n",
    "            weight_decay=self.hparams.weight_decay\n",
    "        )\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=self.hparams.n_warmup_steps,\n",
    "            num_training_steps=self.hparams.n_training_steps\n",
    "        )\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": {\"scheduler\": scheduler, \"interval\": \"step\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best checkpoint path: ../model/best_model_B_minmax_0.2.ckpt\n"
     ]
    }
   ],
   "source": [
    "best_ckpt_path_poetry = '../model/best_model_B_minmax_0.2.ckpt' # Colab 경로에 맞게 수정\n",
    "print(\"Best checkpoint path:\", best_ckpt_path_poetry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "0\n",
      "NVIDIA A100-PCIE-40GB\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())  # True면 GPU 사용 가능\n",
    "print(torch.cuda.device_count())  # 사용 가능한 GPU 개수\n",
    "print(torch.cuda.current_device())  # 현재 활성화된 GPU ID\n",
    "print(torch.cuda.get_device_name(0))  # GPU 이름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0a0+ecf3bae40a.nv25.01 12.8\n",
      "True\n",
      "NVIDIA A100-PCIE-40GB\n",
      "tensor([0.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__, torch.version.cuda)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.zeros(1, device=\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all\n"
     ]
    }
   ],
   "source": [
    "!echo $NVIDIA_DRIVER_CAPABILITIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_poetry = BaseTagger.load_from_checkpoint(best_ckpt_path_poetry, map_location=\"cpu\" )\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "best_model_poetry.to(device)\n",
    "best_model_poetry.eval()\n",
    "best_model_poetry.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_emotion(sample_text):\n",
    "\n",
    "    encoding = tokenizer(\n",
    "        sample_text,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # 입력 텐서 또한 같은 device로 이동\n",
    "        input_ids = encoding[\"input_ids\"].to(device)\n",
    "        attention_mask = encoding[\"attention_mask\"].to(device)\n",
    "\n",
    "        # forward\n",
    "        _, predictions = best_model_poetry(input_ids, attention_mask)  # best_model_poetry 사용\n",
    "\n",
    "    # 추론 결과를 CPU로 가져와 numpy로 변환\n",
    "    predictions = predictions.flatten().cpu().numpy()\n",
    "\n",
    "    # 결과를 딕셔너리로 저장 (숫자값으로 변환)\n",
    "    result_dict = {\n",
    "        label_name: float(round(score, 3))  # np.float32 -> float 변환\n",
    "        for label_name, score in zip(LABELS, predictions)\n",
    "        if score > THRESHOLD\n",
    "    }\n",
    "\n",
    "    return result_dict\n",
    "    # 예시 출력\n",
    "    # {'불안/걱정': 0.336, '슬픔': 0.311}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"\"\"\n",
    "미풍에 웃는 아침을 기원하련다\n",
    "\"\"\" # 송몽규 - 하늘과 더불어"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 샘플 텍스트에 대한 감정분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'환영/호의': 0.6169999837875366,\n",
       " '감동/감탄': 0.40299999713897705,\n",
       " '기대감': 0.9430000185966492,\n",
       " '비장함': 0.3310000002384186,\n",
       " '편안/쾌적': 0.3310000002384186,\n",
       " '아껴주는': 0.6200000047683716,\n",
       " '즐거움/신남': 0.6779999732971191,\n",
       " '흐뭇함(귀여움/예쁨)': 0.32100000977516174,\n",
       " '행복': 0.6949999928474426,\n",
       " '기쁨': 0.8629999756813049}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poem_emotions = classify_emotion(sample_text)\n",
    "poem_emotions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Langchain (Blossom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/bitsandbytes-0.45.4.dev0-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_utilities-0.12.0.dev0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/looseversion-1.3.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/nvfuser-0.2.23a0+6627725-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.0.dev0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: langchain in /home/work/.local/lib/python3.12/site-packages (0.3.27)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /home/work/.local/lib/python3.12/site-packages (from langchain) (0.3.74)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /home/work/.local/lib/python3.12/site-packages (from langchain) (0.3.9)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in /home/work/.local/lib/python3.12/site-packages (from langchain) (0.4.15)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.10.5)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.4.36)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /home/work/.local/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/work/.local/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.12.2)\n",
      "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (23.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /home/work/.local/lib/python3.12/site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /home/work/.local/lib/python3.12/site-packages (from langsmith>=0.1.17->langchain) (0.24.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2024.12.14)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (3.7.1)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (2.3)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.llms import Ollama\n",
    "# from langchain.llms import HuggingFacePipeline\n",
    "# from langchain.prompts import PromptTemplate\n",
    "# from langchain.chains import LLMChain\n",
    "# # from langchain.output_parsers import StrOutputParser\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "# import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Blossom 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_id = \"Bllossom/llama-3.2-Korean-Bllossom-3B\"\n",
    "\n",
    "# # 토크나이저 로드\n",
    "# tokenizer_bllossom = AutoTokenizer.from_pretrained(model_id)\n",
    "# tokenizer_bllossom.pad_token = tokenizer_bllossom.eos_token  # Blossom은 pad_token이 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_id,\n",
    "#     device_map=\"auto\",\n",
    "#     torch_dtype=torch.float16  # 또는 \"auto\"\n",
    "# )\n",
    "\n",
    "# # 2. 텍스트 생성 파이프라인\n",
    "# pipe = pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer_bllossom,\n",
    "#     temperature=0.3, #\t생성의 무작위성 조절 계수\n",
    "#     top_p=0.9, # 누적 확률이 top_p 이하인 토큰들만 고려\n",
    "#     max_new_tokens=4, #한 번에 생성할 최대 토큰 수입니다. (입력 프롬프트 제외)\n",
    "#     repetition_penalty=1.5 # 반복되는 단어에 대한 페널티\n",
    "# )\n",
    "\n",
    "# # 3. LangChain용 LLM 래퍼\n",
    "# llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KOSOLAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6db33651ea074f0796673fcf4c70c0f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "MODEL_ID = \"yanolja/KoSOLAR-10.7B-v0.2\"\n",
    "CACHE_DIR = \"../model/KOSOLAR\"   # 바꿀 경로\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_ID, cache_dir=CACHE_DIR,  use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16, # GPU가 bfloat16/float16 지원 없으면 torch.float32\n",
    "    cache_dir=CACHE_DIR\n",
    ")\n",
    "device = model.device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 프롬프트 엔지니어링 (no Vectior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가벼운,가지런한,간편한,감각적인,감미로운,감성적인,강인한,강한,개성적인,거친,건실한,격식있는,견고한,고급스러운,그윽한,기능적인,기운찬,깊은,깔끔한,나이든,남성적인,넉넉한,다양한,단순한,단정한,달콤한,도시적인,돋보이는,동양적인,동적인,딱딱한,뛰어난,맑은,매끄러운,매력적인,멋진,무거운,밝은,보수적인,복잡한,부드러운,사랑스러운,상쾌한,새로운,서양적인,선명한,섬세한,성숙한,세련된,소박한,수수한,순수한,스포티한,시원한,신선한,실용적인,심플한,싱싱한,아기자기한,안정된,약한,얕은,어두운,여성적인,여유있는,연약한,오래된,와일드한,우울한,유연한,율동적인,이성적인,인공적인,자연적인,자유로운,잔잔한,장식적인,재미있는,전원적인,전통적인,젊은,정다운,정돈된,정적인,조용한,중후한,즐거운,지적인,진보적인,차가운,차분한,친근한,쾌활한,클래식한,탁한,투명한,편리한,편안한,포근한,품위있는,풍성한,하이테크한,한국적인,향기로운,혁신적인,환상적인,활동적인\n"
     ]
    }
   ],
   "source": [
    "# 중복 제거 후 정렬\n",
    "adjectives = sorted(df_IRI['형용사'].dropna().unique())\n",
    "# 배열을 콤마 구분 문자열로 변환\n",
    "adjectives_str = \",\".join(adjectives)\n",
    "\n",
    "print(adjectives_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def score_labels_by_logprob(poem: str, labels: list[str], batch_size: int = 32):\n",
    "    # 프롬프트: '정답:' 뒤를 라벨로 채워넣어 점수화\n",
    "    base_prompt = (\n",
    "        \"너는 시를 읽는 독자야. 너는 주어진 시를 감상하고 시에 가장 의미적으로 어울리는 라벨 중 하나만 고른다\\n\"\n",
    "        f\"허용 라벨: {', '.join(labels)}\\n\\n시:\\n{poem}\\n\\n정답:\"\n",
    "    )\n",
    "    # 고정 프롬프트 길이(토큰)\n",
    "    prompt_ids = tok(base_prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "    prompt_len = prompt_ids.shape[1]\n",
    "\n",
    "    scores = []\n",
    "    # 배치로 후보 점수화 (전체 106개도 충분히 빠름)\n",
    "    for i in range(0, len(labels), batch_size):\n",
    "        batch_labels = labels[i:i+batch_size]\n",
    "        texts = [base_prompt + lab for lab in batch_labels]\n",
    "        enc = tok(texts, return_tensors=\"pt\", padding=True).to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model(**enc)\n",
    "            logits = out.logits  # [B, T, V]\n",
    "            logp = F.log_softmax(logits[:, :-1, :], dim=-1)     # predict next token\n",
    "            next_ids = enc.input_ids[:, 1:]                     # gold next tokens\n",
    "\n",
    "        # 각 샘플별로 '라벨 토큰' 구간만 합산\n",
    "        B, Tm1 = next_ids.shape\n",
    "        for b in range(B):\n",
    "            seq_len = enc.input_ids[b].shape[0]\n",
    "            label_tok_start = prompt_len - 1                    # 첫 예측 위치\n",
    "            label_tok_end   = seq_len - 1                       # 마지막 예측 위치(포함)\n",
    "            idx = torch.arange(Tm1, device=device)\n",
    "            mask = (idx >= label_tok_start) & (idx <= label_tok_end)\n",
    "            row_logp = logp[b].gather(-1, next_ids[b].unsqueeze(-1)).squeeze(-1)\n",
    "            score = row_logp[mask].sum().item()\n",
    "            scores.append((batch_labels[b], score))\n",
    "\n",
    "    # 점수 내림차순\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    print(scores)\n",
    "    return scores\n",
    "\n",
    "def classify_poem(poem: str, labels: list[str]) -> str:\n",
    "    scores = score_labels_by_logprob(poem, labels)\n",
    "    return scores[0][0]\n",
    "\n",
    "# # 예시\n",
    "# poem = \"미풍에 웃는 아칭을 기원하련다\"\n",
    "# print(classify_poem(poem, adjectives))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co-Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8️⃣ 전체 체인\n",
    "def CoReadingPoem(user_input, adjectives):\n",
    "    CoReading_Result = classify_poem(user_input, adjectives)\n",
    "    poem_colors = define_colors(poem_emotions, CoReading_Result , df, IRI_colors_df)\n",
    "    emotion_to_colors([poem_colors['primary_emotion'],poem_colors['secondary_emotion'], CoReading_Result], poem_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 372.00 MiB. GPU 0 has a total capacity of 39.49 GiB of which 43.81 MiB is free. Process 1320 has 29.32 GiB memory in use. Including non-PyTorch memory, this process has 10.12 GiB memory in use. Of the allocated memory 9.00 GiB is allocated by PyTorch, and 630.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mCoReadingPoem\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madjectives\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[56], line 3\u001b[0m, in \u001b[0;36mCoReadingPoem\u001b[0;34m(user_input, adjectives)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mCoReadingPoem\u001b[39m(user_input, adjectives):\n\u001b[0;32m----> 3\u001b[0m     CoReading_Result \u001b[38;5;241m=\u001b[39m \u001b[43mclassify_poem\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madjectives\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     poem_colors \u001b[38;5;241m=\u001b[39m define_colors(poem_emotions, CoReading_Result , df, IRI_colors_df)\n\u001b[1;32m      5\u001b[0m     emotion_to_colors([poem_colors[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprimary_emotion\u001b[39m\u001b[38;5;124m'\u001b[39m],poem_colors[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msecondary_emotion\u001b[39m\u001b[38;5;124m'\u001b[39m], CoReading_Result], poem_colors)\n",
      "Cell \u001b[0;32mIn[55], line 43\u001b[0m, in \u001b[0;36mclassify_poem\u001b[0;34m(poem, labels)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclassify_poem\u001b[39m(poem: \u001b[38;5;28mstr\u001b[39m, labels: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m---> 43\u001b[0m     scores \u001b[38;5;241m=\u001b[39m \u001b[43mscore_labels_by_logprob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m scores[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[0;32mIn[55], line 20\u001b[0m, in \u001b[0;36mscore_labels_by_logprob\u001b[0;34m(poem, labels, batch_size)\u001b[0m\n\u001b[1;32m     18\u001b[0m enc \u001b[38;5;241m=\u001b[39m tok(texts, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 20\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43menc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     logits \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mlogits  \u001b[38;5;66;03m# [B, T, V]\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     logp \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mlog_softmax(logits[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)     \u001b[38;5;66;03m# predict next token\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1740\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1739\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1750\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1753\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1754\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/hooks.py:176\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/llama/modeling_llama.py:1163\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m   1160\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1162\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1163\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1167\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1168\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1169\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1170\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1171\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1172\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1173\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1174\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1177\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1740\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1739\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1750\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1753\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1754\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/llama/modeling_llama.py:913\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    901\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    902\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    903\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         position_embeddings,\n\u001b[1;32m    911\u001b[0m     )\n\u001b[1;32m    912\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 913\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    914\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    925\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    927\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1740\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1739\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1750\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1753\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1754\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/hooks.py:176\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/llama/modeling_llama.py:656\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    654\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    655\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 656\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    657\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    659\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1740\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1739\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1750\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1753\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1754\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/hooks.py:176\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/llama/modeling_llama.py:242\u001b[0m, in \u001b[0;36mLlamaMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 242\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgate_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_proj(x))\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1740\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1739\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1750\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1753\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1754\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/hooks.py:176\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 372.00 MiB. GPU 0 has a total capacity of 39.49 GiB of which 43.81 MiB is free. Process 1320 has 29.32 GiB memory in use. Including non-PyTorch memory, this process has 10.12 GiB memory in use. Of the allocated memory 9.00 GiB is allocated by PyTorch, and 630.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "CoReadingPoem(sample_text, adjectives)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
